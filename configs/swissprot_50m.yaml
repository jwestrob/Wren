# Swiss-Prot training config - 50M param model
model:
  hidden_dim: 512
  num_layers: 8
  num_heads: 8
  head_dim: 64
  ffn_dim: 1365  # ~8/3 * hidden_dim
  vocab_size: 25
  max_seq_len: 2048
  mrl_dims: [64, 128, 256, 512]
  embedding_dim: 512
  dropout: 0.0
  use_bitlinear: true
  use_yarn: true
  rope_scale: 1.0
  gradient_checkpointing: true

training:
  learning_rate: 1.0e-3
  weight_decay: 0.01
  beta1: 0.9
  beta2: 0.95
  eps: 1.0e-8
  max_grad_norm: 1.0
  warmup_steps: 2000
  total_steps: 50000
  lr_schedule: cosine
  batch_size: 8
  gradient_accumulation_steps: 16  # Effective batch = 128
  save_every_steps: 5000
  eval_every_steps: 1000
  checkpoint_dir: checkpoints/swissprot_50m
  log_every_steps: 10
  wandb_project: tinyplm
  device: mps
  mixed_precision: false  # MPS doesn't support AMP well
  seed: 42

data:
  dataset_path: data/swissprot
  max_length: 2048
  min_length: 50
  mask_prob: 0.15
  mask_token_prob: 0.8
  random_token_prob: 0.1
  crop_min_ratio: 0.7
  crop_max_ratio: 0.9
  extra_mask_prob: 0.2
  num_workers: 8
  prefetch_factor: 4
